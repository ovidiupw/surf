Functiile ce au ca scop vizitarea paginilor web sunt grupate, ca stagii de executii paralele, in automatul finit configurat in "AWS Step Functions". Acestea reprezinta executii ale definitiilor formale ale task-urilor de crawling. O parte dintre datele asociate unui task reprezinta metadatele configurate in pasul de creare a workflow-ului. \textit{"Tabelul 2"} prezinta structura unui task, cu mentiunile necesare acolo unde definitia unui camp este exact aceeasi cu definitia campului respectiv din structura unui workflow.

\begin{table}[h]
	\centering
    \begin{tabular}{|M{2.5cm}|M{1.6cm}|M{9cm}|}
    	\hline 
    	Nume camp & Tip & Descriere \\ \hline
    	
    	Id & Text & Identificator unic pentru task \\ \hline
    	
    	Id Workflow & Text & Identificatorul workflow-ului de care apartine task-ul curent \\ \hline
    	
    	Data Incepere & Numeric & Data la care a inceput executia task-ului (engl. epoch milliseconds) \\ \hline
    	
    	Data Finalizare & Numeric & Data la care s-a finalizat executia task-ului(engl. epoch milliseconds) \\ \hline
    	
    	Stare & Text & Starea in care se afla task-ul: \textit{Programat, Activ, Esuat, Depasit temporal, Intrerupt} \\ \hline
    	
		Adresa & Text & URL-ul ce trebuie vizitat in vederea extragerii datelor si metadatelor \\ \hline    	
    	
    	Adancime & Numeric & Nivelul de adancime, in arborele de parcurgere a paginilor web realizat de catre crawler, la care se afla task-ul \\ \hline
    	
    	Erori & Lista & Esecurile inregistrate in decursul executiei task-ului (ca structuri JSON) \\ \hline
    	
    	Politica de selectie & JSON & \textit{vezi Tabelul 1} \\ \hline
    	
    	Dimensiune maxima pagina & Numeric & \textit{vezi Tabelul 1} \\ \hline
    	
    	Selector URL & Text & \textit{vezi Tabelul 1} \\ \hline
    	
    \end{tabular}
\end{table}
\clearpage

\noindent
Procesul de parcurgere a unei pagini web presupune urmatoarele etape:

\begin{enumerate}
	\item{Actualizarea starii task-ului in vederea reflectarii conditiei sale actuale (e.g. "in executie" sau "esuat");}
	
	\item{Verificarea dreptului de acces asupra URL-ului configurat in cadrul definitiei task-ului, prin consultarea fisierului robots.txt (daca acesta exista), aflat in calea de baza (engl. \textit{root}) a sitului web de la URL-ul respectiv, in concordanta cu politica de configurare a procesului de crawling;}
	
	\item{Parcurgerea sitului web:}
	\begin{enumerate}
		\item{Extragerea datelor conform politicii de selectie a datelor (configurata in pasul de creare a workflow-ului) si stocarea acestora (daca exista) in S3;}
		
		\item{Salvarea metadatelor in legatura cu datale extrase, in vederea accesarii sau cautarii facile a locatiei in care au fost salvate in S3;}
		
		\item{Selectarea tuturor legaturilor de tip URL catre alte situri web din cadrul paginii vizitate, conform politicii de urmarire a legaturilor catre alte pagini web, definita in pasul de creare a workflow-ului;}
		
		\item{Crearea de noi task-uri in starea \textit{"Programat"}, configurate cu un nivel de adancime superior celui pe care se afla URL-ul curent parcurs de catre crawler si salvarea acestora in baza de date, doar daca respecta configurarile declarate ca date de intrare ale executiei workflow-ului (e.g. dimensiunea maxima a unei pagini web parcurse); acest pas mai poarta denumirea de \textit{task scheduling\footnote{engl. \textit{task scheduling} = programare temporala a executiei unui task}};}
		
	\end{enumerate}
	
	\item{Actualizarea tabelei in care se tine evidenta paginilor parcurse de catre crawler in timpul executiei unui workflow, cu scopul de a nu se parcurge de doua ori acelasi URL;}
		
	\item{Capturarea, rezolvarea si stocarea erorilor survenite in cadrul procesului de parcurgere a paginilor web.}
	
\end{enumerate}

% Despre erorile in parcurgere