Siturile web pot implementa variate modalitati de contracarare a incercarilor de crawling. Aplicatia "Surf" incearca sa minimizeze riscul de respingere a cererilor de accesare a anumitor resurse web printr-o implementare neintruziva a procesului de crawling. Cateva aspecte esentiale care sunt luate in considerare in ceea ce priveste o astfel de implementare sunt urmatoarele:

\begin{itemize}

	\item{Minimizarea volumului de date preluat de pe un anumit domeniu prin diferite metode de filtrare a linkurilor urmarite (e.g. o anumita structura a URL-ului, un anumit tip de date care se gaseste la URL-ul respectiv);}
	
	\item{Introducerea pauzelor temporale aleatoare intre accesari succesive a datelor apartinand aceluiasi domeniu.}
	
\end{itemize}

\noindent
Crawler-ul web "Surf" poate satisface numeroase cerinte ale utilizatorilor. O parte dintre aceste cerinte poate veni din partea sistemelor anti-malware. In acest caz, nu se doreste respectarea fisierului \emph{robots.txt} (utilizat drept referinta, pentru crawleri, asupra URL-urilor accesibile ale domeniului vizitat), deoarece exita pericolul ca un sit malitios sa blocheze o eventuala scanare. De aceea, aplicatia "Surf" va putea fi configurata in ceea ce priveste ignorarea fisierului \emph{robots.txt} in procesul de parcurgere a unui domeniu.
\\
\\
Crawler-ul "Surf" implementeaza un mecanism de \emph{blacklisting}\footnote{https://en.wikipedia.org/wiki/Blacklisting}. Siturile web sau domeniile care interzic procesul de crawling (e.g. prin ToS\footnote{https://en.wikipedia.org/wiki/Terms\_of\_service}) vor fi adaugate unei liste de excluziune din procesul de parcurgere executat de crawler.
