Natura dinamică World Wide Web-ului, precum şi scara sa, justifică necesitatea existenţei unor mecanisme eficiente de localizare a informaţiilor relevante. Instrumentele specializate în căutarea datelor se bazează pe web crawler-e pentru colectarea informaţiilor din paginile web, ce sunt, ulterior, indexate şi analizate.
\\ 

Crawlerul ajută utilizatorul în ceea ce priveşte navigarea Internetului, automatizând parcurgerea link-urilor. Un crawler exploatează structura web-ului şi extrage resurse, într-o manieră ordonată, ghidat de criteriile specificate de către utilizator. Rezultatul obţinut de pe urma utilizării unui web crawler este o serie de situri web.
\\

În documentele de referinţă, web crawler-ii mai sunt cunoscuţi şi drept "wanderers",  "robots", "spiders", "fish" sau "worms". Aceste denumiri nu sunt indicatori ai performanţei, întrucât aceste programe pot fi foarte rapide şi precise. Se pot parcurge zeci de mii de pagini într-un interval de câteva minute, consumând doar o fracţiune din lungimea de bandă pusă la dispoziţie \cite{GautamPadminiFilippo}, atât timp cât crawler-ul dispune de resursele hardware necesare.
\\ 

Crawler-ii web deservesc unui număr mare de scopuri. Unul dintre acestea este mentenanţa bazei de date în care sunt stocaţi indecşii unui motor de căutare. Pentru această sarcină sunt utilizaţi crawleri de tip exhaustiv (se parcurg toate siturile web întâlnite). O altă categorie este cea a crawler-ilor bazaţi pe conţinut, folosiţi pentru adunarea datelor ce se înscriu sub o anumită tematică, minimizând efortul de a aloca resurse pentru pagini irelevante. În construirea unui astfel crawler este necesară definirea unei modalităţi de căutare şi selecţie a datelor. În cazul crawlerilor bazaţi pe conţinut, pot apărea probleme legate de explorare versus exploatare, deoarece spaţiul de căutare ar putea conţine un optim local, împiedicând algoritmul să localizeze optimul global.\cite{PantSrinivasanMenczer} 
\\