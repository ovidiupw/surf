Natura dinamica World Wide Web-ului, precum si scara sa, justifica necesitatea existentei unor mecanisme eficiente de localizre a informatiilor relevante. Tool-urile specializare in cautarea datelor se bazeaza pe web crawler-e pentru colectarea paginilor web, ce sunt, ulterior, indexate si analizate.
\\ 

Crawlerul ajuta utilizatorul in ceea ce priveste navigarea Internetului, automatizand parcurgerea link-urilor. Un crawler exploateaza structura web-ului \footnote{Web-ul este structurat asemeni unui graf} si extrage resurse, intr-o maniera ordonata, ghidat de criteriile specificate de catre utilizator. Rezultatul obtinut de pe urma utilizarii unui web crawler este o serie de situri web.
\\

In documentele de referinta, web crawler-ii mai sunt cunoscuti si drept "wanderers",  "robots", "spiders", "fish" sau "worms". Aceste denumiri nu sunt indicatori ai performantei, intrucat aceste programe pot fi foarte rapide si precise. Se pot parcurge zeci de mii de pagini intr-un interval de cateva minute, consumand doar o fractiune din lungimea de banda pusa la dispozitie \cite{GautamPadminiFilippo}, atat timp cat crawler-ul dispune de resursele hardware minime necesare.
\\ 

Crawler-ii web deservesc unui numar mare de scopuri. Unul dintre acestea este mentenanta bazei de date in care sunt stocati indecsii unui motor de cautare. Pentru aceasta sarcina sunt utilizati crawleri de tip exhaustiv (se parcurg toate siturile web intalnite). O alta categorie este cea a crawler-ilor bazati pe euristici, folositi pentru adunarea datelor ce se inscriu sub o anumita tematica, minimizand efortul de a aloca resurse pentru pagini irelevante. In construirea unui astfel crawler este necesara definirea unei modalitati de cautare si selectie a datelor. In cazul crawlerilor bazati pe euristici, pot aparea probleme legate de explorare versus exploatare, deoarece spatiul de cautare ar putea contine un optim local, impiedicand algoritmul sa localizeze optimul global.\cite{PantSrinivasanMenczer} 
\\