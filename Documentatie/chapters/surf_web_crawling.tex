\newcommand{\robotsTxtDescription}{http://www.robotstxt.org/robotstxt.html}

Ca sens general, un crawler web reprezintă un program care parcurge, prin cereri succesive, situri web. Programatic, vom considera următoarele aspecte cheie în proiectarea unui crawler distribuit:

\begin{enumerate}

	\item{Punctele de pornire în parcurgerea siturilor web;}
	
	\item{Adâncimea maximă a parcurgerii recursive a siturilor (i.e. cea mai îndepărtată pagină la care se poate ajunge, de la punctul de pornire, prin accesarea succesivă a legaturilor de tipul hyperlink;}
	
	\item{Politica de selecție a informațiilor din paginile parcurse\cite{web-crawler-selection-policy};}
	
	\item{Viteza de crawling, fișierul \emph{robots.txt}\footnote{\robotsTxtDescription} și respectarea drepturilor de autor;}
	
	\item{Politica de paralelizare a procesului de web crawling;}
	
	\item{Politica de retenție temporară a rezultatelor parcurgerii siturilor web și jurnalizare a acțiunilor serviciului de crawling;}
	
\end{enumerate}

În cele ce urmează, se va descrie particularizarea aspectelor generale enumerate mai sus în cadrul serviciului web "Surf". Se va crea, astfel, contextul dezvoltării aplicației și se vor puncta principalele componente funcționale implicate.
