Datorită ritmului alert în care se creează sau schimbă informaţiile aparţinând world wide web-ului, există o necesitate tot mai mare de a putea prelua şi analiza datele într-un mod automat, rapid şi eficient. Crawlerii web sunt o componentă deosebit de importantă, în vederea extragerii şi indexării exhaustive a informaţiilor, pentru motoarele de căutare. Cu toate acestea, avantajele aduse de către crawleri pot fi folosite şi pentru a extrage resurse relevante dintr-un anumit context, fără a fi necesară o parcurgere a tuturor resurselor web accesibile. Odată cu apariţia şi dezvoltarea serviciilor web, infrastructura puternică şi costurile reduse oferite de către furnizorii cloud oferă modalităţi flexibile, intuitive şi extensibile de a susţine crawleri specializaţi. 
\\

\noindent
Crawler-ul web "Surf" propune, în acest sens, o modalitate simplă pentru ca un utilizator interesat de resurse dintr-o anumită arie să poată să le preia automat şi să le analizeze conţinutul, fără a fi necesar să depindă de furnizori terţi de servicii web de crawling şi cu posibilitatea de ajustare minuţioasă a costurilor operaţionale. Paralelizarea sarcinilor de parcurgere a paginilor web, împreună cu scalabilitatea orizontală şi verticală asigurată de servciile cloud, garantează adaptabilitatea aplicaţiei "Surf" la cele mai diverse nevoi ale utilizatorilor săi. Flexibilitatea legată atât de costuri, cât şi de design-ul arhitectural al aplicaţiei "Surf" alături de API-ul RESTful, care expune funcţionalităţile într-un mod accesibil şi extensibil, permite dezvoltatorilor interesaţi să adauge elemente suplimentare, precum: componente de tip map/reduce, analiză semantică a conţinutului paginilor web şi tehnici euristice pentru selectarea frontierei de URL-uri. De asemenea, flexibilitatea serviciilor cloud garantează atingerea unui nivel de activitate de aproape 100\%, oferind siguranţă şi încredere în utilizare.